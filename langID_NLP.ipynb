{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of langID_NLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dagobert42/langID-NLP/blob/main/langID_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt1fyVMmyyc5"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import string\r\n",
        "import re\r\n",
        "from nltk import ngrams\r\n",
        "from collections import defaultdict\r\n",
        "import nltk\r\n",
        "import collections\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcHt-U5X0Qk0"
      },
      "source": [
        "# read data\r\n",
        "# this was written for the WiLI-2018 data set: https://zenodo.org/record/841984\r\n",
        "# make sure txt-files are in the specified directory when running this \r\n",
        "X_train = open('x_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "Y_train = open('y_train.txt', encoding=\"utf8\").read().split('\\n')\r\n",
        "labels = pd.read_csv('labels.csv', delimiter = ';')\r\n",
        "Y_train =  Y_train[:-1]\r\n",
        "X_train =  X_train[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgaeru6_AWpW"
      },
      "source": [
        "# remove unnecessary characters from data\r\n",
        "extras = '!\"$%&/{}[]()=?\\\\`´*+~#-_.:,;<>|1234567890°-\\'' # Characters to remove from data\r\n",
        "rx = '[' + re.escape(''.join(extras)) + ']'\r\n",
        "x_train = [] \r\n",
        "for example in X_train:\r\n",
        "    x_train.append(re.sub(' +', ' ', re.sub(rx, '', example)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1WIZ_bAkZA"
      },
      "source": [
        "# convert language labels to language Name => 'en' -> 'English'\r\n",
        "lab_dict = { labels.loc[i]['Label'] : labels.loc[i]['English'] for i in range(0, len(labels)) }\r\n",
        "y_train = [ lab_dict[item] if item != 'nan' else 'Min Nan Chinese' for item in Y_train ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNBMCn3bA078"
      },
      "source": [
        "# ordering sentences by language\r\n",
        "lang_corpora = defaultdict(list)\r\n",
        "for i in range(len(x_train)):\r\n",
        "    lang_corpora[y_train[i]].append(x_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_gdP0c8ZGtJ"
      },
      "source": [
        "# creating n-grams for each language\r\n",
        "# data has to be a dict of lang : corpus\r\n",
        "# returns a dict of lang : n-grams\r\n",
        "def n_grams_per_lang(n, data):\r\n",
        "    gram_per_lang = defaultdict(list)\r\n",
        "    for lang in data.keys():\r\n",
        "        for sent in data[lang]:\r\n",
        "            gram_per_lang[lang] += [sent[i:i+n] for i in range(len(sent)-n+1)]\r\n",
        "    \r\n",
        "    return gram_per_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT5Bp8t6ZJyz"
      },
      "source": [
        "# counting and sorting n-grams for each language\r\n",
        "# data has to be a dict of lang : n-grams\r\n",
        "# returns a sorted dict of lang : {n-gram : count}\r\n",
        "def sort_by_tf(data):\r\n",
        "\r\n",
        "    # calculating term frequency of n-grams per language\r\n",
        "    tf_per_lang = defaultdict(list)\r\n",
        "    for lang in data.keys():\r\n",
        "        tf_per_lang[lang] = dict(zip(list(collections.Counter(data[lang]).keys()), list(collections.Counter(data[lang]).values())))\r\n",
        "\r\n",
        "    # sort by term frequency\r\n",
        "    sorted_tf_per_lang = defaultdict(list)\r\n",
        "    for lang in data.keys():\r\n",
        "        sorted_tf_per_lang[lang] = { word : value for word, value in sorted(tf_per_lang[lang].items(), key=lambda item:item[1], reverse=True) }\r\n",
        "    \r\n",
        "    return sorted_tf_per_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2m7X2FGbQwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d6f2e5-af92-420e-fef2-8cda403c27cf"
      },
      "source": [
        "# make some n-grams and print examples\n",
        "for n in range(3, 6):\n",
        "    ngpl = n_grams_per_lang(n, lang_corpora)\n",
        "    sorted_tf_per_lang = sort_by_tf(ngpl)\n",
        "\n",
        "    # print some examples\n",
        "    languages = ['German', 'English', 'Arabic']\n",
        "    n_samples = 10\n",
        "    for lang_key in languages:\n",
        "        print(lang_key, ':', n, '- grams')\n",
        "        print(list(sorted_tf_per_lang[lang_key].keys())[:n_samples])\n",
        "        print(list(sorted_tf_per_lang[lang_key].values())[:n_samples])\n",
        "    print('##########################')\n",
        "\n",
        "    latin_languages = ['German', 'English', 'French', 'Spanish', 'Italian', 'Portugese', 'Estonian',\n",
        "                        'Turkish', 'Romanian', 'Swedish', 'Latin', 'Dutch']\n",
        "    ng_related = {}\n",
        "    \n",
        "    for lang_key in latin_languages:\n",
        "        for otherlang in latin_languages:\n",
        "            top20 = list(sorted_tf_per_lang[lang_key].keys())[:n_samples]\n",
        "            if otherlang == lang_key:\n",
        "                continue\n",
        "            else:\n",
        "                top20x = list(sorted_tf_per_lang[otherlang].keys())[:n_samples]\n",
        "                # compares the two top 20 lists for common elements:\n",
        "                common_ngrams = list(set(top20).intersection(top20x))\n",
        "            \n",
        "                \n",
        "                if len(common_ngrams) > 0:\n",
        "                    ng_related[lang_key] = otherlang\n",
        "                \n",
        "\n",
        "    print('common '+n+ '- grams dictionary: ')\n",
        "    print(ng_related)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German : 3 - grams\n",
            "['en ', 'er ', ' de', 'der', 'sch', 'ie ', 'che', 'nd ', 'ein', 'ch ']\n",
            "[3915, 3021, 2241, 1655, 1473, 1336, 1184, 1175, 1101, 1073]\n",
            "English : 3 - grams\n",
            "[' th', 'he ', 'the', 'ed ', ' in', ' an', 'nd ', 'and', ' of', 'of ']\n",
            "[2838, 2765, 2569, 1660, 1371, 1326, 1287, 1277, 1240, 1182]\n",
            "Arabic : 3 - grams\n",
            "[' ال', 'الم', 'ية ', 'في ', ' في', 'ة ا', ' من', 'من ', 'ن ا', 'ات ']\n",
            "[8365, 1595, 1468, 1441, 1424, 1378, 1168, 1086, 1084, 937]\n",
            "##########################\n",
            "German : 4 - grams\n",
            "['der ', ' der', 'und ', ' die', ' und', 'den ', 'die ', 'ten ', 'sche', ' ein']\n",
            "[1316, 1137, 886, 863, 845, 800, 782, 687, 656, 631]\n",
            "English : 4 - grams\n",
            "[' the', 'the ', ' of ', 'and ', ' and', ' in ', 'ing ', ' to ', 'tion', 'ion ']\n",
            "[2368, 2149, 1179, 1149, 1067, 926, 900, 763, 630, 519]\n",
            "Arabic : 4 - grams\n",
            "[' الم', 'ة ال', ' في ', 'ن ال', ' من ', ' وال', 'ي ال', 'ت ال', ' الأ', ' الت']\n",
            "[1359, 1300, 1289, 1015, 948, 696, 686, 626, 611, 603]\n",
            "##########################\n",
            "German : 5 - grams\n",
            "[' der ', ' und ', ' die ', 'ische', 'chen ', ' von ', ' den ', ' eine', 'n der', 'schen']\n",
            "[1118, 844, 776, 522, 488, 470, 429, 394, 365, 346]\n",
            "English : 5 - grams\n",
            "[' the ', ' and ', 'n the', ' was ', 'tion ', ' of t', 'of th', 'ation', 'f the', ' for ']\n",
            "[2147, 1065, 449, 448, 408, 390, 368, 367, 359, 332]\n",
            "Arabic : 5 - grams\n",
            "[' على ', 'ات ال', 'في ال', ' في ا', 'من ال', ' من ا', 'ية ال', ' إلى ', 'ة في ', 'لى ال']\n",
            "[478, 456, 443, 421, 382, 366, 358, 323, 271, 264]\n",
            "##########################\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}